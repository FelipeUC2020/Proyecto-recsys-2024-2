{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import pandas as pd\n",
    "from obp.dataset import OpenBanditDataset\n",
    "from obp.ope import OffPolicyEvaluation, InverseProbabilityWeighting as IPW\n",
    "from obp.policy import BernoulliTS\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation, \n",
    "    RegressionModel,\n",
    "    DirectMethod,\n",
    "    InverseProbabilityWeighting,\n",
    "    DoublyRobust\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import time\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versión automática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explorar_clusters(n_clusters, particion):\n",
    "\n",
    "    # Importar dataset original desde una copia: 'women_copy.csv'\n",
    "    path = os.path.join(\"open_bandit_dataset\", \"random\", \"women\", \"women_copy.csv\")\n",
    "    dataset = pd.read_csv(path,  index_col=0) # por ahora solo se utilizan datos 'women', 'random'\n",
    "\n",
    "    print(\"Tamaño del dataset completo: \", dataset.shape)\n",
    "\n",
    "    full_dataset = dataset.copy()\n",
    "\n",
    "    # user_features = dataset[[\"user_feature_0\", \"user_feature_1\", \"user_feature_2\", \"user_feature_3\"]]\n",
    "    # user_features_encoded = pd.get_dummies(user_features, columns=[\"user_feature_0\", \"user_feature_1\", \"user_feature_2\", \"user_feature_3\"])\n",
    "\n",
    "    user_features_encoded =  dataset.drop(columns=[\"item_id\", \"position\", \"click\", \"propensity_score\", \"timestamp\", \"user_feature_0\", \"user_feature_1\", \"user_feature_2\", \"user_feature_3\"])\n",
    "\n",
    "    kmeans = KMeans(init=\"k-means++\", n_clusters=n_clusters, random_state=12345)\n",
    "    predict = kmeans.fit_predict(user_features_encoded)\n",
    "\n",
    "    full_dataset['clusters'] = predict\n",
    "\n",
    "    cluster_datasets = []\n",
    "    cluster_results = []\n",
    "    cluster_sizes = []\n",
    "    bandit_dataset = []\n",
    "    # Aplicar una mascara al dataset para filtrar por cluster\n",
    "    for i in range(n_clusters): \n",
    "        print(i)\n",
    "        cluster_datasets.append(full_dataset.loc[full_dataset['clusters'] == i])\n",
    "\n",
    "    # Iteracion principal para sobreescribir csv de OBP\n",
    "    for i in range(n_clusters):\n",
    "\n",
    "        ################# potencial problema ###############\n",
    "        path = os.path.join(\"open_bandit_dataset\", \"random\", \"women\", \"women.csv\")\n",
    "        dataset = cluster_datasets[i]\n",
    "        cluster_size = dataset.shape[0]\n",
    "\n",
    "        dataset.to_csv(path, index=False)\n",
    "        print(\"Tamaño del dataser de cluster \", i, \": \" , dataset.shape)\n",
    "        ####################################################\n",
    "\n",
    "        dataset = OpenBanditDataset(behavior_policy=\"random\", campaign=\"women\", data_path=\"open_bandit_dataset\", dataset_name=\"women.csv\")\n",
    "        bandit_feedback = dataset.obtain_batch_bandit_feedback()\n",
    "\n",
    "        bandit_dataset.append(dataset)\n",
    "        \n",
    "        ######### Aqui aplicar simulacion y estimacion sobre clusters ###########\n",
    "        evaluation_policy = BernoulliTS(\n",
    "            n_actions=dataset.n_actions, \n",
    "            len_list=dataset.len_list, \n",
    "            campaign=\"women\",\n",
    "            random_state=12345,\n",
    "            policy_name = \"random\"\n",
    "        )\n",
    "\n",
    "        regression_model = RegressionModel(\n",
    "            n_actions=dataset.n_actions,\n",
    "            len_list=dataset.len_list,\n",
    "            action_context=dataset.action_context,\n",
    "            base_model=LogisticRegression(max_iter=1000, random_state=12345),\n",
    "        )\n",
    "\n",
    "        estimated_rewards_by_reg_model = regression_model.fit_predict(\n",
    "            context=bandit_feedback[\"context\"],\n",
    "            action=bandit_feedback[\"action\"],\n",
    "            reward=bandit_feedback[\"reward\"],\n",
    "            position=bandit_feedback[\"position\"],\n",
    "            pscore=bandit_feedback[\"pscore\"],\n",
    "            n_folds=3, # use 3-fold cross-fitting\n",
    "            random_state=12345,\n",
    "        )\n",
    "        \n",
    "        action_dist = evaluation_policy.compute_batch_action_dist(\n",
    "            n_sim=100000, n_rounds=bandit_feedback[\"n_rounds\"],\n",
    "        )\n",
    "\n",
    "        # estimate the policy value of BernoulliTS based on its action choice probabilities\n",
    "    # it is possible to set multiple OPE estimators to the `ope_estimators` arguwoment\n",
    "        ope = OffPolicyEvaluation(\n",
    "            bandit_feedback=bandit_feedback,\n",
    "            ope_estimators=[InverseProbabilityWeighting(), DirectMethod(), DoublyRobust()]\n",
    "        )\n",
    "        \n",
    "\n",
    "        # `summarize_off_policy_estimates` returns pandas dataframes including the OPE results\n",
    "        estimated_policy_value, estimated_interval = ope.summarize_off_policy_estimates(\n",
    "            action_dist=action_dist, \n",
    "            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n",
    "            n_bootstrap_samples=10000, # number of resampling performed in bootstrap sampling.\n",
    "            random_state=12345,\n",
    "        )\n",
    "\n",
    "        cluster_results.append(estimated_policy_value)\n",
    "        cluster_sizes.append(cluster_size)\n",
    "    return cluster_results, cluster_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- Explorando con 2 cluster(s) -----------------------\n",
      "\n",
      "Tamaño del dataset completo:  (864585, 55)\n",
      "0\n",
      "1\n",
      "Tamaño del dataser de cluster  0 :  (860888, 56)\n",
      "Tamaño del dataser de cluster  1 :  (3697, 56)\n",
      "{2: [860888, 3697]}\n",
      "\n",
      "---------------------- Explorando con 3 cluster(s) -----------------------\n",
      "\n",
      "Tamaño del dataset completo:  (864585, 55)\n",
      "0\n",
      "1\n",
      "2\n",
      "Tamaño del dataser de cluster  0 :  (857155, 56)\n",
      "Tamaño del dataser de cluster  1 :  (3535, 56)\n",
      "Tamaño del dataser de cluster  2 :  (3895, 56)\n",
      "{2: [860888, 3697], 3: [857155, 3535, 3895]}\n",
      "\n",
      "---------------------- Explorando con 4 cluster(s) -----------------------\n",
      "\n",
      "Tamaño del dataset completo:  (864585, 55)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Tamaño del dataser de cluster  0 :  (854191, 56)\n",
      "Tamaño del dataser de cluster  1 :  (3472, 56)\n",
      "Tamaño del dataser de cluster  2 :  (3880, 56)\n",
      "Tamaño del dataser de cluster  3 :  (3042, 56)\n",
      "{2: [860888, 3697], 3: [857155, 3535, 3895], 4: [854191, 3472, 3880, 3042]}\n",
      "\n",
      "---------------------- Explorando con 5 cluster(s) -----------------------\n",
      "\n",
      "Tamaño del dataset completo:  (864585, 55)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Tamaño del dataser de cluster  0 :  (850543, 56)\n",
      "Tamaño del dataser de cluster  1 :  (3406, 56)\n",
      "Ejecución detenida en cluster_size:  5\n",
      "\n",
      "------------------------------ Resultados ------------------------------\n",
      "\n",
      "{2: [     estimated_policy_value  relative_estimated_policy_value\n",
      "ipw                0.004809                         1.000708\n",
      "dm                 0.004823                         1.003604\n",
      "dr                 0.004808                         1.000467,      estimated_policy_value  relative_estimated_policy_value\n",
      "ipw                0.007077                         1.006356\n",
      "dm                 0.007018                         0.997929\n",
      "dr                 0.006931                         0.985477], 3: [     estimated_policy_value  relative_estimated_policy_value\n",
      "ipw                0.004786                         1.000732\n",
      "dm                 0.004802                         1.004066\n",
      "dr                 0.004785                         1.000541,      estimated_policy_value  relative_estimated_policy_value\n",
      "ipw                0.007118                         1.006443\n",
      "dm                 0.007171                         1.014034\n",
      "dr                 0.007209                         1.019420,      estimated_policy_value  relative_estimated_policy_value\n",
      "ipw                0.009995                         0.998247\n",
      "dm                 0.010074                         1.006148\n",
      "dr                 0.009848                         0.983564], 4: [     estimated_policy_value  relative_estimated_policy_value\n",
      "ipw                0.004735                         1.000646\n",
      "dm                 0.004739                         1.001529\n",
      "dr                 0.004734                         1.000401,      estimated_policy_value  relative_estimated_policy_value\n",
      "ipw                0.007247                         1.006443\n",
      "dm                 0.007030                         0.976389\n",
      "dr                 0.007343                         1.019757,      estimated_policy_value  relative_estimated_policy_value\n",
      "ipw                0.009779                         0.998503\n",
      "dm                 0.009584                         0.978554\n",
      "dr                 0.009649                         0.985228,      estimated_policy_value  relative_estimated_policy_value\n",
      "ipw                0.019191                         1.006551\n",
      "dm                 0.019584                         1.027134\n",
      "dr                 0.019495                         1.022488]}\n"
     ]
    }
   ],
   "source": [
    "cluster_sizes = [2, 3, 4, 5, 10, 15]\n",
    "\n",
    "cluster_results = {}       # resultados OPE\n",
    "cluster_size_results = {}  # tamanos de cada cluster evaluado\n",
    "\n",
    "for cluster_size in cluster_sizes: \n",
    "    print(f\"\\n---------------------- Explorando con {cluster_size} cluster(s) -----------------------\\n\")\n",
    "    try:\n",
    "\n",
    "        experiwoment_results, experiwoment_size_results = explorar_clusters(n_clusters=cluster_size, particion='women')\n",
    "        cluster_results[cluster_size] = experiwoment_results\n",
    "        cluster_size_results[cluster_size] = experiwoment_size_results\n",
    "        print(cluster_size_results)\n",
    "    except: \n",
    "        print(\"Ejecución detenida en cluster_size: \", cluster_size)\n",
    "        break\n",
    "print(f\"\\n------------------------------ Resultados ------------------------------\\n\")\n",
    "print(cluster_results)\n",
    "\n",
    "# Para guardar los resultados de estimadores\n",
    "with open('results_women_user_affinity.pkl', 'wb') as f:\n",
    "    pickle.dump(cluster_results, f)\n",
    "\n",
    "# Para guardar los tamaños de los clusters    \n",
    "with open('size_results_women_user_affinity.pkl', 'wb') as f: \n",
    "    pickle.dump(cluster_size_results, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
