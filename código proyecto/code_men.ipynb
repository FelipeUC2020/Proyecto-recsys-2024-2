{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import pandas as pd\n",
    "from obp.dataset import OpenBanditDataset\n",
    "from obp.ope import OffPolicyEvaluation, InverseProbabilityWeighting as IPW\n",
    "from obp.policy import BernoulliTS\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation, \n",
    "    RegressionModel,\n",
    "    DirectMethod,\n",
    "    InverseProbabilityWeighting,\n",
    "    DoublyRobust\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import time\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versión automática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explorar_clusters(n_clusters, particion):\n",
    "\n",
    "    # Importar dataset original desde una copia: 'men_copy.csv'\n",
    "    path = os.path.join(\"open_bandit_dataset\", \"random\", \"men\", \"men_copy.csv\")\n",
    "    dataset = pd.read_csv(path,  index_col=0) # por ahora solo se utilizan datos 'men', 'random'\n",
    "\n",
    "    print(\"Tamaño del dataset completo: \", dataset.shape)\n",
    "\n",
    "    full_dataset = dataset.copy()\n",
    "\n",
    "    \n",
    "    user_features = dataset[[\"user_feature_0\", \"user_feature_1\", \"user_feature_2\", \"user_feature_3\"]]\n",
    "    user_features_encoded = pd.get_dummies(user_features, columns=[\"user_feature_0\", \"user_feature_1\", \"user_feature_2\", \"user_feature_3\"])\n",
    "\n",
    "    # user_featuresk_encoded =  dataset.drop(columns=[\"item_id\", \"position\", \"click\", \"propensity_score\", \"timestamp\", \"user_feature_0\", \"user_feature_1\", \"user_feature_2\", \"user_feature_3\"])\n",
    "\n",
    "    kmeans = KMeans(init=\"k-means++\", n_clusters=n_clusters, random_state=12345)\n",
    "    predict = kmeans.fit_predict(user_features_encoded)\n",
    "\n",
    "    full_dataset['clusters'] = predict\n",
    "\n",
    "    cluster_datasets = []\n",
    "    cluster_results = []\n",
    "    cluster_sizes = []\n",
    "    bandit_dataset = []\n",
    "    # Aplicar una mascara al dataset para filtrar por cluster\n",
    "    for i in range(n_clusters): \n",
    "        print(i)\n",
    "        cluster_datasets.append(full_dataset.loc[full_dataset['clusters'] == i])\n",
    "\n",
    "    # Iteracion principal para sobreescribir csv de OBP\n",
    "    for i in range(n_clusters):\n",
    "\n",
    "        ################# potencial problema ###############\n",
    "        path = os.path.join(\"open_bandit_dataset\", \"random\", \"men\", \"men.csv\")\n",
    "        dataset = cluster_datasets[i]\n",
    "        cluster_size = dataset.shape[0]\n",
    "\n",
    "        dataset.to_csv(path, index=False)\n",
    "        print(\"Tamaño del dataser de cluster \", i, \": \" , dataset.shape)\n",
    "        ####################################################\n",
    "\n",
    "        dataset = OpenBanditDataset(behavior_policy=\"random\", campaign=\"men\", data_path=\"open_bandit_dataset\", dataset_name=\"men.csv\")\n",
    "        bandit_feedback = dataset.obtain_batch_bandit_feedback()\n",
    "\n",
    "        bandit_dataset.append(dataset)\n",
    "        \n",
    "        ######### Aqui aplicar simulacion y estimacion sobre clusters ###########\n",
    "        evaluation_policy = BernoulliTS(\n",
    "            n_actions=dataset.n_actions, \n",
    "            len_list=dataset.len_list, \n",
    "            campaign=\"men\",\n",
    "            random_state=12345,\n",
    "            policy_name = \"random\"\n",
    "        )\n",
    "\n",
    "        regression_model = RegressionModel(\n",
    "            n_actions=dataset.n_actions,\n",
    "            len_list=dataset.len_list,\n",
    "            action_context=dataset.action_context,\n",
    "            base_model=LogisticRegression(max_iter=1000, random_state=12345),\n",
    "        )\n",
    "\n",
    "        estimated_rewards_by_reg_model = regression_model.fit_predict(\n",
    "            context=bandit_feedback[\"context\"],\n",
    "            action=bandit_feedback[\"action\"],\n",
    "            reward=bandit_feedback[\"reward\"],\n",
    "            position=bandit_feedback[\"position\"],\n",
    "            pscore=bandit_feedback[\"pscore\"],\n",
    "            n_folds=3, # use 3-fold cross-fitting\n",
    "            random_state=12345,\n",
    "        )\n",
    "        \n",
    "        action_dist = evaluation_policy.compute_batch_action_dist(\n",
    "            n_sim=100000, n_rounds=bandit_feedback[\"n_rounds\"],\n",
    "        )\n",
    "\n",
    "        # estimate the policy value of BernoulliTS based on its action choice probabilities\n",
    "    # it is possible to set multiple OPE estimators to the `ope_estimators` argument\n",
    "        ope = OffPolicyEvaluation(\n",
    "            bandit_feedback=bandit_feedback,\n",
    "            ope_estimators=[InverseProbabilityWeighting(), DirectMethod(), DoublyRobust()]\n",
    "        )\n",
    "        \n",
    "\n",
    "        # `summarize_off_policy_estimates` returns pandas dataframes including the OPE results\n",
    "        estimated_policy_value, estimated_interval = ope.summarize_off_policy_estimates(\n",
    "            action_dist=action_dist, \n",
    "            estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n",
    "            n_bootstrap_samples=10000, # number of resampling performed in bootstrap sampling.\n",
    "            random_state=12345,\n",
    "        )\n",
    "\n",
    "        cluster_results.append(estimated_policy_value)\n",
    "        cluster_sizes.append(cluster_size)\n",
    "    return cluster_results, cluster_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- Explorando con 2 cluster(s) -----------------------\n",
      "\n",
      "Tamaño del dataset completo:  (452949, 43)\n",
      "0\n",
      "1\n",
      "Tamaño del dataser de cluster  0 :  (132305, 44)\n",
      "Tamaño del dataser de cluster  1 :  (320644, 44)\n",
      "{2: [132305, 320644]}\n",
      "\n",
      "------------------------------ Resultados ------------------------------\n",
      "\n",
      "{2: [     estimated_policy_value  relative_estimated_policy_value\n",
      "ipw                0.006684                         0.999250\n",
      "dm                 0.006685                         0.999434\n",
      "dr                 0.006681                         0.998800,      estimated_policy_value  relative_estimated_policy_value\n",
      "ipw                0.004473                         0.998786\n",
      "dm                 0.004509                         1.006815\n",
      "dr                 0.004474                         0.998899]}\n"
     ]
    }
   ],
   "source": [
    "# cluster_sizes = [2, 3, 4, 5, 10, 15]\n",
    "cluster_sizes = [2]\n",
    "\n",
    "cluster_results = {}       # resultados OPE\n",
    "cluster_size_results = {}  # tamanos de cada cluster evaluado\n",
    "\n",
    "for cluster_size in cluster_sizes: \n",
    "    print(f\"\\n---------------------- Explorando con {cluster_size} cluster(s) -----------------------\\n\")\n",
    "    try:\n",
    "\n",
    "        experiment_results, experiment_size_results = explorar_clusters(n_clusters=cluster_size, particion='men')\n",
    "        cluster_results[cluster_size] = experiment_results\n",
    "        cluster_size_results[cluster_size] = experiment_size_results\n",
    "        print(cluster_size_results)\n",
    "    except: \n",
    "        print(\"Ejecución detenida en cluster_size: \", cluster_size)\n",
    "        break\n",
    "print(f\"\\n------------------------------ Resultados ------------------------------\\n\")\n",
    "print(cluster_results)\n",
    "\n",
    "# Para guardar los resultados de estimadores\n",
    "with open('results_men_user_affinity.pkl', 'wb') as f:\n",
    "    pickle.dump(cluster_results, f)\n",
    "\n",
    "# Para guardar los tamaños de los clusters    \n",
    "with open('size_results_men_user_affinity.pkl', 'wb') as f: \n",
    "    pickle.dump(cluster_size_results, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
