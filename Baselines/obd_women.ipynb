{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart Example with Open Bandit Dataset\n",
    "---\n",
    "This notebook demonstrates an example of conducting OPE of Bernoulli Thompson Sampling (BernoulliTS) as an evaluation policy. We use some OPE estimators and logged bandit data generated by running the Random policy (behavior policy) on the ZOZOTOWN platform. We also evaluate and compare the OPE performance (accuracy) of several estimators.\n",
    "\n",
    "The example consists of the following four major steps:\n",
    "- (1) Data Loading and Preprocessing\n",
    "- (2) Replicating Production Policy\n",
    "- (3) Off-Policy Evaluation (OPE)\n",
    "- (4) Evaluation of OPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed when using Google Colab\n",
    "# !pip install obp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "from obp.dataset import OpenBanditDataset\n",
    "from obp.policy import BernoulliTS\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation, \n",
    "    RegressionModel,\n",
    "    DirectMethod,\n",
    "    InverseProbabilityWeighting,\n",
    "    DoublyRobust\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.5\n"
     ]
    }
   ],
   "source": [
    "# obp version\n",
    "print(obp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Data Loading and Preprocessing\n",
    "\n",
    "`obp.dataset.OpenBanditDataset` is an easy-to-use data loader for Open Bandit Dataset. \n",
    "\n",
    "It takes behavior policy ('bts' or 'random') and campaign ('all', 'men', or 'women') as inputs and provides dataset preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load and preprocess raw data in \"All\" campaign collected by the Random policy (behavior policy here)\n",
    "# When `data_path` is not given, this class downloads the small-sized version of the Open Bandit Dataset.\n",
    "dataset = OpenBanditDataset(behavior_policy='random', campaign='women', data_path=\"open_bandit_dataset\", dataset_name=\"women.csv\")\n",
    "\n",
    "# obtain logged bandit feedback generated by behavior policy\n",
    "bandit_feedback = dataset.obtain_batch_bandit_feedback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the logged bandit dataset is collected by the behavior policy as follows.\n",
    "\n",
    "$ \\mathcal{D}_b := \\{(x_i,a_i,r_i)\\}$  where $(x,a,r) \\sim p(x)\\pi_b(a | x)p(r | x,a) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n_rounds', 'n_actions', 'action', 'position', 'reward', 'pscore', 'context', 'action_context'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `bandit_feedback` is a dictionary storing logged bandit feedback\n",
    "bandit_feedback.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's see some properties of the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'women.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of the dataset is 'obd' (open bandit dataset)\n",
    "dataset.dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of actions of the \"All\" campaign is 80\n",
    "dataset.n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "864585"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small sample example data has 10,000 samples (or rounds)\n",
    "dataset.n_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default context (feature) engineering creates context vector with 20 dimensions\n",
    "dataset.dim_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ZOZOTOWN recommendation interface has three positions\n",
    "# (please see https://github.com/st-tech/zr-obp/blob/master/images/recommended_fashion_items.png)\n",
    "dataset.len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Replicating Production Policy\n",
    "\n",
    "After preparing the dataset, we now replicate the BernoulliTS policy implemented on the ZOZOTOWN recommendation interface during the data collection period.\n",
    "\n",
    "Here, we use `obp.policy.BernoulliTS` as an evaluation policy. \n",
    "By activating its `is_zozotown_prior` argument, we can replicate (the policy parameters of) BernoulliTS used in the ZOZOTOWN production.\n",
    "\n",
    "(When `is_zozotown_prior=False`, non-informative prior distribution is used.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define BernoulliTS as an evaluation policy\n",
    "evaluation_policy = BernoulliTS(\n",
    "    n_actions=dataset.n_actions, \n",
    "    len_list=dataset.len_list, \n",
    "    is_zozotown_prior=True, # replicate the BernoulliTS policy in the ZOZOTOWN production\n",
    "    campaign=\"women\",\n",
    "    random_state=12345,\n",
    ")\n",
    "\n",
    "# compute the action choice probabilities of the evaluation policy via Monte Carlo simulation\n",
    "action_dist = evaluation_policy.compute_batch_action_dist(\n",
    "    n_sim=100000, n_rounds=bandit_feedback[\"n_rounds\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.03021, 0.01222, 0.00911],\n",
       "        [0.0019 , 0.00104, 0.0007 ],\n",
       "        [0.00883, 0.03448, 0.07574],\n",
       "        ...,\n",
       "        [0.00229, 0.00209, 0.00206],\n",
       "        [0.02412, 0.02397, 0.02458],\n",
       "        [0.06974, 0.09191, 0.09871]],\n",
       "\n",
       "       [[0.03021, 0.01222, 0.00911],\n",
       "        [0.0019 , 0.00104, 0.0007 ],\n",
       "        [0.00883, 0.03448, 0.07574],\n",
       "        ...,\n",
       "        [0.00229, 0.00209, 0.00206],\n",
       "        [0.02412, 0.02397, 0.02458],\n",
       "        [0.06974, 0.09191, 0.09871]],\n",
       "\n",
       "       [[0.03021, 0.01222, 0.00911],\n",
       "        [0.0019 , 0.00104, 0.0007 ],\n",
       "        [0.00883, 0.03448, 0.07574],\n",
       "        ...,\n",
       "        [0.00229, 0.00209, 0.00206],\n",
       "        [0.02412, 0.02397, 0.02458],\n",
       "        [0.06974, 0.09191, 0.09871]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.03021, 0.01222, 0.00911],\n",
       "        [0.0019 , 0.00104, 0.0007 ],\n",
       "        [0.00883, 0.03448, 0.07574],\n",
       "        ...,\n",
       "        [0.00229, 0.00209, 0.00206],\n",
       "        [0.02412, 0.02397, 0.02458],\n",
       "        [0.06974, 0.09191, 0.09871]],\n",
       "\n",
       "       [[0.03021, 0.01222, 0.00911],\n",
       "        [0.0019 , 0.00104, 0.0007 ],\n",
       "        [0.00883, 0.03448, 0.07574],\n",
       "        ...,\n",
       "        [0.00229, 0.00209, 0.00206],\n",
       "        [0.02412, 0.02397, 0.02458],\n",
       "        [0.06974, 0.09191, 0.09871]],\n",
       "\n",
       "       [[0.03021, 0.01222, 0.00911],\n",
       "        [0.0019 , 0.00104, 0.0007 ],\n",
       "        [0.00883, 0.03448, 0.07574],\n",
       "        ...,\n",
       "        [0.00229, 0.00209, 0.00206],\n",
       "        [0.02412, 0.02397, 0.02458],\n",
       "        [0.06974, 0.09191, 0.09871]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `action_dist` is an array of shape (n_rounds, n_actions, len_list) \n",
    "# representing the distribution over actions by the evaluation policy\n",
    "action_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Off-Policy Evaluation (OPE)\n",
    "Our next step is OPE, which aims to estimate the performance of evaluation policies using logged bandit data and OPE estimators.\n",
    "\n",
    "Here, we use \n",
    "- `obp.ope.InverseProbabilityWeighting` (IPW)\n",
    "- `obp.ope.DirectMethod` (DM)\n",
    "- `obp.ope.DoublyRobust` (DR)\n",
    "\n",
    "as estimators and visualize the OPE results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-1) Obtaining a reward estimator\n",
    "A reward estimator $\\hat{q}(x,a)$ is needed for model dependent estimators such as DM or DR.\n",
    "\n",
    "$\\hat{q}(x,a) \\approx \\mathbb{E} [r \\mid x,a]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the expected rewards by using an ML model (Logistic Regression here)\n",
    "# the estimated rewards are used by model-dependent estimators such as DM and DR\n",
    "regression_model = RegressionModel(\n",
    "    n_actions=dataset.n_actions,\n",
    "    len_list=dataset.len_list,\n",
    "    action_context=dataset.action_context,\n",
    "    base_model=LogisticRegression(max_iter=1000, random_state=12345),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_rewards_by_reg_model = regression_model.fit_predict(\n",
    "    context=bandit_feedback[\"context\"],\n",
    "    action=bandit_feedback[\"action\"],\n",
    "    reward=bandit_feedback[\"reward\"],\n",
    "    position=bandit_feedback[\"position\"],\n",
    "    pscore=bandit_feedback[\"pscore\"],\n",
    "    n_folds=3, # use 3-fold cross-fitting\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "please refer to https://arxiv.org/abs/2002.08536 about the details of the cross-fitting procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-2) Off-Policy Evaluation\n",
    "$V(\\pi_e) \\approx \\hat{V} (\\pi_e; \\mathcal{D}_b, \\theta)$ using DM, IPW, and DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimate the policy value of BernoulliTS based on its action choice probabilities\n",
    "# it is possible to set multiple OPE estimators to the `ope_estimators` argument\n",
    "ope = OffPolicyEvaluation(\n",
    "    bandit_feedback=bandit_feedback,\n",
    "    ope_estimators=[InverseProbabilityWeighting(), DirectMethod(), DoublyRobust()]\n",
    ")\n",
    "\n",
    "# `summarize_off_policy_estimates` returns pandas dataframes including the OPE results\n",
    "estimated_policy_value, estimated_interval = ope.summarize_off_policy_estimates(\n",
    "    action_dist=action_dist, \n",
    "    estimated_rewards_by_reg_model=estimated_rewards_by_reg_model,\n",
    "    n_bootstrap_samples=10000, # number of resampling performed in bootstrap sampling.\n",
    "    random_state=12345,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimated_policy_value</th>\n",
       "      <th>relative_estimated_policy_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ipw</th>\n",
       "      <td>0.006813</td>\n",
       "      <td>1.481190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dm</th>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.990160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dr</th>\n",
       "      <td>0.006863</td>\n",
       "      <td>1.491972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     estimated_policy_value  relative_estimated_policy_value\n",
       "ipw                0.006813                         1.481190\n",
       "dm                 0.004555                         0.990160\n",
       "dr                 0.006863                         1.491972"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the estimated policy value of the evaluation policy (the BernoulliTS policy)\n",
    "# relative_estimated_policy_value is the policy value of the evaluation policy \n",
    "# relative to the ground-truth policy value of the behavior policy (the Random policy here)\n",
    "estimated_policy_value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
